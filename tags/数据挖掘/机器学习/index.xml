<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数据挖掘/机器学习 on Kuhung&#39;s Blog</title>
    <link>https://kuhungio.me/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 数据挖掘/机器学习 on Kuhung&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    
	<atom:link href="https://kuhungio.me/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>从 QQ 扫描用户数据，到 LIME 与 SHAP 两类模型解释方法的比较</title>
      <link>https://kuhungio.me/2021/interpretable-machine-learning/</link>
      <pubDate>Fri, 22 Jan 2021 15:24:12 +0800</pubDate>
      
      <guid>https://kuhungio.me/2021/interpretable-machine-learning/</guid>
      <description>背景 QQ 的“安全”策略 2021年1月18日，QQ 系被爆出扫描用户的浏览器历史记录，并对特定关键词进行了记录。
QQ 的公关回应说，这是安全策略。但其实，懂的都懂，这里不做过分推演。
某种意义上讲，目前各大互联网企业的竞争，实质上是数据的竞争。
数据越多越好 作为调参从业人员，从算法和模型角度来说，数据确实是越多越好。
数据越多，模型能捕获到的有用特征则更多。从特征状态空间，映射到实际标签的过程，会更顺畅、更准确。
我的数据谁做主 为什么大家会这么关注这个问题？有两方面的原因：
一：采集行为未经用户授权，且数据采集毫不相关。
二：日常生活中，有太多”精准“的推送。
早上刚和同事说想买 PS5，晚上各大 app 就开始展示 PS5 的商品广告。
用户可不管你用的什么方法，你拿了我的无关数据，展示了我心里想的东西。
啪的一下，两件事情就得到了关联。
现象背后的解释 心理的谬误 从心理学来讲，确实存在“孕妇效应”和“幸存者偏差”，会让我们错误地把两件事归因在一起。
即，怀孕的人，会突然发现，大街上怀孕的人变多了。但实际变多了吗？其实并没有，只是之前的注意力没在这里而已。
”幸存者“，则是那些发声的人。广告虽然会尽量投放给潜在顾客，但刚好命中前一秒有需求的，是少数。发声出来，让大家以为这是普遍现象。
算法也有“恐怖谷” 更多的则是，从用户体验层面，无理由的精准，着实会招致抵触。
在数据应用的早期，大家觉得这是个新鲜好玩的东西。但当推荐越来越离谱，仿佛读心术一般的，数据的“恐怖谷”效应也就出现了。
这和仿生人领域一样。当机器人越来越像人，人类感受到的不是亲切，而是害怕。推送太准确了，以至于让人发怵，从生理上抵触。
我模型牛逼，要什么解释性 而在研发层面，在做模型时，往往容易陷入参数狂热。
随着各种模型方法的支持，我们很容易将各类脑洞、各类特征、各类技巧糅合在一起，做出一个看起来还不错的结果。
但其实，我们很难解释，是哪部分带来的效果。是深度学习的网络结构，还是交叉几轮找不着北的特征，抑或是那不起眼的坐标信息？
大多数情况，我们给不了解释。
同行与监管的挑战 为了解决数据滥用、模型黑盒问题，业界正在发生变化：
软件提供者层面 iOS在14的版本更新中，有一个最显著的变化，引起了我的注意：系统在做应用推荐时，会给出推荐的理由。
而这，在此前的版本都是没有的。
例如连上蓝牙耳机时，屏幕的下方会显示：推荐xx音乐，因为连上蓝牙时经常这么做。
监管层面 蚂蚁上市告吹，马已经服。金融时报这样评价：
 同时，大数据、人工智能等技术易导致“算法歧视”，严重损害特殊群体利益。相较于传统歧视行为，算法歧视更难约束。
其一，算法歧视维度多元。传统歧视行为通常依据性别、学历等显著外在特征，但算法能挖掘更深层次的隐形特征作为依据。
其二，算法歧视形式隐蔽。基于种族、性别、民族等特征的歧视行为被法律禁止，但自动化决策可利用“算法的不可解释性”规避职责，在不触犯现有法律规定的情况下，侵犯消费者合法权益。
尤其是当某一个大型互联网企业拥有涉及数亿消费者天量数据信息的情况下，即使从个体和逐笔看，其数据来源和使用均获得了消费者授权，但从总体看，可能存在“合成的谬误”，这些数据在总体上具有公共品性质，其管理、运用并非单一消费者授权就能解决其合法性问题。
 这说明，监管已经注意到这个层面。且他们的认知很专业，抓住了问题的核心：无法解释的算法，隐藏在超参数背后的歧视，将会侵犯我们每个人的权益。
这次他们没有“喝茶看报打哈哈”。
数据来源需合法合规，模型解释也应该有所依据。
模型的可解释性，必须提上议程。
 下面内容比较生涩，我尽量做到深入浅出，小白看了懂大概，同行看了知方法。
我们需要可解释性 可解释的3大必要性   对于用户：用户需要知道，你有没有“偷窥”他的隐私，是否有私底下采集用户数据，用于歧视性定价等。
这会影响用户忠诚度和品牌美誉度。
如果总是很“精准”地推一些东西，用神经网络或者是组合特征、泛人群特征，用户实际是会很懵逼的。
信息不对称的情况下，他会直接联想到最近的行为，从而产生被监视的感觉。
  对于监管，监管部门有责任掌握细节，防止信息的滥用与风险的滋生。
就像针对蚂蚁金服的调查：用户的多维度数据被企业用来谋求更大利益，滋生出巨大的金融系统性风险。
  对于业务，业务方需要知道每个模型背后的原理，以便更好的做出决策。</description>
    </item>
    
  </channel>
</rss>